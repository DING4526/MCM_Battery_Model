# experiments/exp_compare.py
# åœºæ™¯å¯¹æ¯”å®éªŒæ¨¡å—ï¼ˆPlotly ç‰ˆæœ¬ï¼‰

import sys
import os
import numpy as np

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from simulate import run_simulation, run_monte_carlo
from visualization.comparison import (
    plot_scenario_comparison,
    plot_scenario_boxplot,
    plot_scenario_radar,
)
from visualization.config import save_plotly_figure, get_output_dir
from usage.scenario import *


SCENARIO_GROUPS = {
    "æ—¥å¸¸åœºæ™¯": {
        "å­¦ç”Ÿæ—¥å¸¸": SCENARIO_STUDENT_DAILY_MIXED,
        "é€šå‹¤": SCENARIO_COMMUTE_MIXED,
        "å‘¨æœ«å¨±ä¹": SCENARIO_WEEKEND_MIXED,
        "æ—…è¡Œ": SCENARIO_TRAVEL_MIXED,
    },
    "æç«¯åœºæ™¯": {
        "çº¯å¾…æœº": PURE_DEEPIDLE,
        "çº¯ç¤¾äº¤": PURE_SOCIAL,
        "çº¯è§†é¢‘": PURE_VIDEO,
        "çº¯æ¸¸æˆ": PURE_GAMING,
        "çº¯å¯¼èˆª": PURE_NAVIGATION,
    },
    "æ··åˆ vs Markov": {
        "å­¦ç”Ÿæ—¥å¸¸ Mixed": SCENARIO_STUDENT_DAILY_MIXED,
        "å­¦ç”Ÿæ—¥å¸¸ Markov": SCENARIO_STUDENT_DAILY_MARKOV,
        "é€šå‹¤ Mixed": SCENARIO_COMMUTE_MIXED,
        "é€šå‹¤ Markov": SCENARIO_COMMUTE_MARKOV,
    },
}


def run_comparison_experiment(
    scenarios=None,
    group_name=None,
    n_mc=100,
    base_seed=0,
    dt=1.0,
    T_amb=298.15,
    verbose=True,
    output_dir="compare",
):
    """è¿è¡Œåœºæ™¯å¯¹æ¯”å®éªŒ"""
    
    if scenarios is None:
        if group_name is not None and group_name in SCENARIO_GROUPS:
            scenarios = SCENARIO_GROUPS[group_name]
        else:
            scenarios = SCENARIO_GROUPS["æ—¥å¸¸åœºæ™¯"]
            group_name = "æ—¥å¸¸åœºæ™¯"
    
    if verbose:
        print("=" * 60)
        print("ğŸ”¬ åœºæ™¯å¯¹æ¯”å®éªŒ")
        print("=" * 60)
        print(f"åœºæ™¯ç»„: {group_name}")
        print(f"Monte Carlo æ ·æœ¬æ•°: {n_mc}")
        print(f"ç¯å¢ƒæ¸©åº¦: {T_amb - 273.15:.1f} Â°C")
        print("-" * 60)
    
    results = {}
    
    for name, scenario in scenarios.items():
        if verbose:
            print(f"åˆ†æåœºæ™¯: {name}...")
        
        ttl_list = run_monte_carlo(
            scenario=scenario,
            n_samples=n_mc,
            base_seed=base_seed,
            dt=dt,
            T_amb=T_amb,
        )
        
        results[name] = {
            "ttl_list": ttl_list,
            "mean": np.mean(ttl_list),
            "std": np.std(ttl_list),
            "min": np.min(ttl_list),
            "max": np.max(ttl_list),
            "median": np.median(ttl_list),
            "q1": np.percentile(ttl_list, 25),
            "q3": np.percentile(ttl_list, 75),
        }
        
        if verbose:
            print(f"  å‡å€¼: {results[name]['mean']/3600:.2f} h")
    
    if verbose:
        print("-" * 60)
        print("âœ… åœºæ™¯å¯¹æ¯”å®Œæˆï¼")
        sorted_scenarios = sorted(results.keys(), key=lambda s: results[s]["mean"], reverse=True)
        for i, name in enumerate(sorted_scenarios, 1):
            print(f"  {i}. {name}: {results[name]['mean']/3600:.2f} h")
        print("=" * 60)
    
    # ä¿å­˜å›¾è¡¨ï¼ˆPlotly ç‰ˆæœ¬ï¼‰
    if verbose:
        print("ä¿å­˜å›¾è¡¨...")
    
    # 1. åœºæ™¯å¯¹æ¯”æŸ±çŠ¶å›¾
    fig = plot_scenario_comparison(results, show=False)
    save_plotly_figure(fig, "scenario_comparison", output_dir, size_type="default")
    
    # 2. åœºæ™¯ç®±çº¿å›¾
    fig = plot_scenario_boxplot(results, show=False)
    save_plotly_figure(fig, "scenario_boxplot", output_dir, size_type="default")
    
    # 3. é›·è¾¾å›¾
    fig = plot_scenario_radar(results, show=False)
    save_plotly_figure(fig, "scenario_radar", output_dir, size_type="square")
    
    if verbose:
        out_path = get_output_dir(output_dir)
        print(f"å›¾è¡¨å·²ä¿å­˜åˆ° {out_path}/ ç›®å½•")
    
    return results


def run_sensitivity_to_temperature(
    scenario=None,
    scenario_name="é»˜è®¤åœºæ™¯",
    T_amb_range=None,
    n_mc=50,
    verbose=True,
    output_dir="compare",
):
    """è¿è¡Œæ¸©åº¦æ•æ„Ÿæ€§åˆ†æï¼ˆPlotly ç‰ˆæœ¬ï¼‰"""
    import plotly.graph_objects as go
    from visualization.config import COLORS, LINE_WIDTHS, FONT_SIZES, FIGURE_SIZES
    
    if scenario is None:
        scenario = SCENARIO_STUDENT_DAILY_MIXED
        scenario_name = "å­¦ç”Ÿæ—¥å¸¸"
    
    if T_amb_range is None:
        T_amb_range = [273.15 + t for t in [0, 10, 20, 25, 30, 35, 40]]
    
    if verbose:
        print("ğŸŒ¡ï¸ ç¯å¢ƒæ¸©åº¦æ•æ„Ÿæ€§åˆ†æ")
    
    results = {"temperatures": [], "means": [], "stds": []}
    
    for T_amb in T_amb_range:
        ttl_list = run_monte_carlo(scenario=scenario, n_samples=n_mc, T_amb=T_amb)
        results["temperatures"].append(T_amb - 273.15)
        results["means"].append(np.mean(ttl_list) / 3600)
        results["stds"].append(np.std(ttl_list) / 3600)
    
    fig = go.Figure()
    
    # å‡å€¼çº¿
    fig.add_trace(go.Scatter(
        x=results["temperatures"],
        y=results["means"],
        mode='lines+markers',
        name='å‡å€¼',
        line=dict(color=COLORS["accent"], width=LINE_WIDTHS["main"]),
        marker=dict(size=8),
    ))
    
    # è¯¯å·®å¸¦
    upper = [m + s for m, s in zip(results["means"], results["stds"])]
    lower = [m - s for m, s in zip(results["means"], results["stds"])]
    
    fig.add_trace(go.Scatter(
        x=results["temperatures"] + results["temperatures"][::-1],
        y=upper + lower[::-1],
        fill='toself',
        fillcolor='rgba(41, 128, 185, 0.2)',
        line=dict(color='rgba(255,255,255,0)'),
        name='Â±1Ïƒ',
        showlegend=True,
    ))
    
    width, height = FIGURE_SIZES["default"]
    fig.update_layout(
        title=dict(
            text=f"ç¯å¢ƒæ¸©åº¦å¯¹ç»­èˆªæ—¶é—´çš„å½±å“ - {scenario_name}",
            font=dict(size=FONT_SIZES["title"]),
        ),
        xaxis_title="ç¯å¢ƒæ¸©åº¦ (Â°C)",
        yaxis_title="ç»­èˆªæ—¶é—´ TTL (å°æ—¶)",
        width=width,
        height=height,
        legend=dict(font=dict(size=FONT_SIZES["legend"])),
        margin=dict(l=50, r=20, t=50, b=45),
    )
    
    save_plotly_figure(fig, "temperature_sensitivity", output_dir, size_type="default")
    
    return results


if __name__ == "__main__":
    run_comparison_experiment()
